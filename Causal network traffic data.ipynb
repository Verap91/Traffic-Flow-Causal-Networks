{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d2daf2",
   "metadata": {},
   "source": [
    "- network building\n",
    "- circular visualization\n",
    "- bar plot node strenght\n",
    "- save adiacence matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effa917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folium network\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "\n",
    "# Ensure plots directory exists\n",
    "plots_dir = 'plots'\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('0.SpeedCT_Hr.csv')\n",
    "\n",
    "# Load COOR data\n",
    "coor_data = pd.read_csv('CoorTraffic.csv')\n",
    "\n",
    "# Extract the start time from the 'Time' range\n",
    "data['StartTime'] = data['Time'].str.split('-').str[0]\n",
    "\n",
    "# Combine 'Date' and 'StartTime' to create 'DateTime'\n",
    "data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['StartTime'], format='%d.%m.%y %H:%M')\n",
    "\n",
    "# Drop the original 'Date' and 'Time' columns\n",
    "data.drop(columns=['Date', 'Time', 'StartTime'], inplace=True)\n",
    "\n",
    "# Set the maximum lag\n",
    "max_lag = 1  # Change lag\n",
    "\n",
    "# Filter for Tuesdays, Wednesdays, and Thursdays\n",
    "data['DayOfWeek'] = data['DateTime'].dt.dayofweek\n",
    "filtered_data = data[data['DayOfWeek'].isin([6])]  # 0=Monday, 1=Tuesday, 2=Wednesday, 3=Thursday, 4=Friday\n",
    "\n",
    "# Filter for specific hours (e.g., 6 AM to 10 AM)\n",
    "#filtered_data['Hour'] = filtered_data['DateTime'].dt.hour\n",
    "#filtered_data = filtered_data[(filtered_data['Hour'] >= 6) & (filtered_data['Hour'] <= 10)]\n",
    "\n",
    "# Handle duplicates by averaging the speeds for the same DateTime and ID\n",
    "filtered_data = filtered_data.groupby(['DateTime', 'ID']).agg({'H.Avg Speed': 'mean'}).reset_index()\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag):\n",
    "    try:\n",
    "        dataframe = dataframe.dropna(subset=[column1, column2])\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[max_lag][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[max_lag][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to create a Folium network map\n",
    "def create_folium_network_map(title, filename, node_positions, outlink_strength_total, inlink_strength_total):\n",
    "    # Initialize the map centered around the mean coordinates\n",
    "    map_center = [coor_data['NORD'].mean(), coor_data['EST'].mean()]\n",
    "    m = folium.Map(location=map_center, zoom_start=13)\n",
    "\n",
    "    # Normalize outlink and inlink strengths using the total strengths\n",
    "    outlink_min, outlink_max = min(outlink_strength_total.values()), max(outlink_strength_total.values())\n",
    "    inlink_min, inlink_max = min(inlink_strength_total.values()), max(inlink_strength_total.values())\n",
    "\n",
    "    outlink_strength_normalized = {station: (outlink_strength_total[station] - outlink_min) / (outlink_max - outlink_min) if outlink_max != outlink_min else 0 for station in outlink_strength_total}\n",
    "    inlink_strength_normalized = {station: (inlink_strength_total[station] - inlink_min) / (inlink_max - inlink_min) if inlink_max != inlink_min else 0 for station in inlink_strength_total}\n",
    "\n",
    "    # Setup color normalization and colormap\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "    cmap = plt.cm.viridis\n",
    "\n",
    "    # Add nodes to the map\n",
    "    for station, (x, y) in node_positions.items():\n",
    "        if station in outlink_strength_normalized and station in inlink_strength_normalized:\n",
    "            size = 2 + outlink_strength_normalized[station] * 6  # Node size based on outlink strength\n",
    "            color = to_hex(cmap(norm(inlink_strength_normalized[station])))  # Node color based on inlink strength\n",
    "            marker = folium.CircleMarker(location=[y, x], radius=size, color=color, fill=True, fill_color=color, fill_opacity=0.6, popup=station) if coor_data[coor_data['IDDI'] == station]['Direction'].values[0] == 'IN' else folium.RegularPolygonMarker(location=[y, x], radius=size, color=color, fill=True, fill_color=color, fill_opacity=0.6, popup=station, number_of_sides=5)\n",
    "            marker.add_to(m)\n",
    "\n",
    "    # Save the map\n",
    "    m.save(filename)\n",
    "    print(f\"Saved map to {filename}\")\n",
    "\n",
    "# Create node positions dictionary for plotting using the full coor_data\n",
    "node_positions = {row['IDDI']: (row['EST'], row['NORD']) for _, row in coor_data.iterrows()}\n",
    "\n",
    "# Initialize total strengths\n",
    "outlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "inlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "\n",
    "# Pivot the filtered data\n",
    "pivot_data = filtered_data.pivot(index='DateTime', columns='ID', values='H.Avg Speed').fillna(0)\n",
    "\n",
    "# Analysis for each station pair\n",
    "for station1 in pivot_data.columns:\n",
    "    for station2 in pivot_data.columns:\n",
    "        if station1 != station2:\n",
    "            f_statistic_out = granger_test(pivot_data, station1, station2, max_lag)\n",
    "            if f_statistic_out > 0:\n",
    "                outlink_strength_total[station1] += f_statistic_out\n",
    "                inlink_strength_total[station2] += f_statistic_out\n",
    "\n",
    "# Create network map\n",
    "title = 'Granger Causality Network'\n",
    "filename = f'{plots_dir}/3.AvgSpeedHr_Sun.html'\n",
    "create_folium_network_map(title, filename, node_positions, outlink_strength_total, inlink_strength_total)\n",
    "\n",
    "# Print total strengths\n",
    "print(\"Total Outlink Strength:\", outlink_strength_total)\n",
    "print(\"Total Inlink Strength:\", inlink_strength_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a26797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# circlular network\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import os\n",
    "\n",
    "# Ensure plots directory exists\n",
    "plots_dir = 'plots'\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('0.TrafficCT.csv')\n",
    "\n",
    "# Load COOR data\n",
    "coor_data = pd.read_csv('CoorTraffic.csv')\n",
    "\n",
    "# Extract the start time from the 'Time' range\n",
    "data['StartTime'] = data['Time'].str.split('-').str[0]\n",
    "\n",
    "# Combine 'Date' and 'StartTime' to create 'DateTime'\n",
    "data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['StartTime'], format='%d.%m.%y %H:%M')\n",
    "\n",
    "# Drop the original 'Date' and 'Time' columns\n",
    "data.drop(columns=['Date', 'Time', 'StartTime'], inplace=True)\n",
    "\n",
    "# Set the maximum lag\n",
    "max_lag = 1  # Change lag\n",
    "\n",
    "# Filter for specific day (e.g., Monday)\n",
    "data['DayOfWeek'] = data['DateTime'].dt.dayofweek\n",
    "filtered_data = data[data['DayOfWeek'].isin([6])]  # 0=Monday\n",
    "\n",
    "# Handle duplicates by averaging the speeds for the same DateTime and ID\n",
    "filtered_data = filtered_data.groupby(['DateTime', 'ID']).agg({'Congestion Factor': 'mean'}).reset_index()\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag):\n",
    "    try:\n",
    "        dataframe = dataframe.dropna(subset=[column1, column2])\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[max_lag][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[max_lag][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize total strengths\n",
    "outlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "inlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "\n",
    "# Pivot the filtered data\n",
    "pivot_data = filtered_data.pivot(index='DateTime', columns='ID', values='Congestion Factor').fillna(0)\n",
    "\n",
    "# Create a graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Collect all F statistics\n",
    "f_statistics = []\n",
    "\n",
    "# Analysis for each station pair and add edges to the graph\n",
    "edges = []\n",
    "for station1 in pivot_data.columns:\n",
    "    for station2 in pivot_data.columns:\n",
    "        if station1 != station2:\n",
    "            f_statistic_out = granger_test(pivot_data, station1, station2, max_lag)\n",
    "            if f_statistic_out > 0:\n",
    "                outlink_strength_total[station1] += f_statistic_out\n",
    "                inlink_strength_total[station2] += f_statistic_out\n",
    "                edges.append((station1, station2, f_statistic_out))\n",
    "                f_statistics.append(f_statistic_out)\n",
    "\n",
    "# Filter edges for the top ToT% F-statistics\n",
    "threshold = np.percentile([f for _, _, f in edges], 95)\n",
    "filtered_edges = [(u, v, f) for u, v, f in edges if f >= threshold]\n",
    "\n",
    "# Add nodes and filtered edges to the graph\n",
    "G.add_nodes_from(pivot_data.columns)\n",
    "G.add_weighted_edges_from(filtered_edges)\n",
    "\n",
    "# Create a circular layout\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "# Normalize edge weights for color mapping\n",
    "norm = Normalize(vmin=min(f for _, _, f in filtered_edges), vmax=max(f for _, _, f in filtered_edges))\n",
    "\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=100, node_color='lightblue')\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_color='black')\n",
    "nx.draw_networkx_edges(G, pos, edgelist=filtered_edges, edge_color=[to_hex(cmap(norm(f))) for _, _, f in filtered_edges], arrowstyle='-|>', arrowsize=15, width=0.6)\n",
    "\n",
    "# Create colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.set_label('F-statistic')\n",
    "\n",
    "# Save the plot\n",
    "#plt.title('Granger Causality Network for Mondays')\n",
    "plt.savefig(f'{plots_dir}/3.Circular_CongestionFactor_Sun.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram of F statistics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(f_statistics, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of F Statistics - Congestion Factor')\n",
    "plt.xlabel('F Statistic')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the histogram as a JPG image with 300 dpi\n",
    "plt.savefig(f'{plots_dir}/3.f_statistics_histogramCongestionFactor_Sun.jpg', format='jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf78d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "# Ensure plots directory exists\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('0.SpeedCT_Hr.csv')\n",
    "\n",
    "# Load COOR data\n",
    "coor_data = pd.read_csv('CoorTraffic.csv')\n",
    "\n",
    "# Extract the start time from the 'Time' range\n",
    "data['StartTime'] = data['Time'].str.split('-').str[0]\n",
    "\n",
    "# Combine 'Date' and 'StartTime' to create 'DateTime'\n",
    "data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['StartTime'], format='%d.%m.%y %H:%M')\n",
    "\n",
    "# Drop the original 'Date', 'Time', and 'StartTime' columns\n",
    "data.drop(columns=['Date', 'Time', 'StartTime'], inplace=True)\n",
    "\n",
    "# Filter for specific days (e.g., Monday to Friday)\n",
    "data['DayOfWeek'] = data['DateTime'].dt.dayofweek\n",
    "filtered_data = data[data['DayOfWeek'].isin([6])]  # 0=Monday, 1=Tuesday, 2=Wednesday, 3=Thursday, 4=Friday\n",
    "\n",
    "# Handle duplicates by averaging the speeds for the same DateTime and ID\n",
    "filtered_data = filtered_data.groupby(['DateTime', 'ID']).agg({'H.Avg Speed': 'mean'}).reset_index()\n",
    "\n",
    "# Set the maximum lag\n",
    "max_lag = 1  # Change lag\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag):\n",
    "    try:\n",
    "        dataframe = dataframe.dropna(subset=[column1, column2])\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[max_lag][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[max_lag][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Loop over the entire dataset for analysis\n",
    "outlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "inlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "\n",
    "# Pivot the filtered data\n",
    "pivot_data = filtered_data.pivot(index='DateTime', columns='ID', values='H.Avg Speed').fillna(0)\n",
    "\n",
    "links = {}\n",
    "outlink_strength = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "inlink_strength = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "\n",
    "# Analysis for each station pair\n",
    "for station1 in pivot_data.columns:\n",
    "    for station2 in pivot_data.columns:\n",
    "        if station1 != station2:\n",
    "            f_statistic_out = granger_test(pivot_data, station1, station2, max_lag)\n",
    "            if f_statistic_out > 0:\n",
    "                links[(station1, station2)] = f_statistic_out\n",
    "                outlink_strength[station1] += f_statistic_out\n",
    "                inlink_strength[station2] += f_statistic_out\n",
    "\n",
    "# Update total strengths\n",
    "for station in filtered_data['ID'].unique():\n",
    "    outlink_strength_total[station] += outlink_strength[station]\n",
    "    inlink_strength_total[station] += inlink_strength[station]\n",
    "\n",
    "# Merge COOR data with data to get coordinates\n",
    "merged_data = pd.merge(filtered_data, coor_data, left_on='ID', right_on='IDDI', how='left')\n",
    "\n",
    "# Function to plot total strengths and save the plot with specified colors for IN and OUT nodes\n",
    "def plot_total_strengths(total_strength, title, ylabel, filename, dpi=300):\n",
    "    sorted_nodes = sorted(total_strength.items(), key=lambda x: x[1], reverse=True)\n",
    "    node_ids = [node[0] for node in sorted_nodes]\n",
    "    node_strengths = [node[1] for node in sorted_nodes]\n",
    "\n",
    "    # Ensure all node IDs have names\n",
    "    node_names = []\n",
    "    for node in node_ids:\n",
    "        name_row = merged_data[merged_data['ID'] == node]\n",
    "        if not name_row.empty:\n",
    "            node_names.append(name_row['Name'].iloc[0])\n",
    "        else:\n",
    "            node_names.append(f'Node {node}')\n",
    "\n",
    "    # Plot total strengths\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(np.arange(len(node_names)), node_strengths, color=['red' if merged_data[merged_data['ID'] == node]['Direction'].iloc[0] == 'OUT' else 'blue' for node in node_ids])\n",
    "\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.xticks(np.arange(len(node_names)), node_names, rotation=90, ha='right', fontsize=4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create legend for IN and OUT nodes\n",
    "    legend_labels = {'OUT': 'red', 'IN': 'blue'}\n",
    "\n",
    "    plt.legend(handles=[Line2D([0, 1], [0, 0], color=color, linewidth=3) for color in legend_labels.values()],\n",
    "               labels=list(legend_labels.keys()), loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Save plot with specified DPI\n",
    "    plt.savefig(filename, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plot total outlink strengths for the entire dataset and save the plot\n",
    "outlink_filename = 'plots/3.total_outlink_strengthAvgSpeedHr_Sun.jpg'\n",
    "plot_total_strengths(outlink_strength_total, 'Total Outlink Strength per Node - H.Avg Speed Hr', 'Total Outlink Strength', outlink_filename)\n",
    "\n",
    "# Plot total inlink strengths for the entire dataset and save the plot\n",
    "inlink_filename = 'plots/3.total_intlink_strengtAvgSpeedHr_Sun.jpg'\n",
    "plot_total_strengths(inlink_strength_total, 'Total Inlink Strength per Node - H.Avg Speed Hr', 'Total Inlink Strength', inlink_filename)\n",
    "\n",
    "# Print total strengths\n",
    "print(\"Total Outlink Strength:\", outlink_strength_total)\n",
    "print(\"Total Inlink Strength:\", inlink_strength_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "# Ensure plots directory exists\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('0.SpeedCT_15m.csv')\n",
    "\n",
    "# Load COOR data\n",
    "coor_data = pd.read_csv('CoorTraffic.csv')\n",
    "\n",
    "# Extract the start time from the 'Time' range\n",
    "data['StartTime'] = data['Time'].str.split('-').str[0]\n",
    "\n",
    "# Combine 'Date' and 'StartTime' to create 'DateTime'\n",
    "data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['StartTime'], format='%d.%m.%y %H:%M')\n",
    "\n",
    "# Drop the original 'Date', 'Time', and 'StartTime' columns\n",
    "data.drop(columns=['Date', 'Time', 'StartTime'], inplace=True)\n",
    "\n",
    "# Filter for specific days (e.g., Tuesday, Wednesday, and Thursday)\n",
    "data['DayOfWeek'] = data['DateTime'].dt.dayofweek\n",
    "filtered_data = data[data['DayOfWeek'].isin([0,1,2,3,4])]  # 0=Monday, 1=Tuesday, 2=Wednesday, 3=Thursday\n",
    "\n",
    "\n",
    "# Filter for specific hours (e.g., 6 AM to 10 AM)\n",
    "#filtered_data['Hour'] = filtered_data['DateTime'].dt.hour\n",
    "#filtered_data = filtered_data[(filtered_data['Hour'] >= 6) & (filtered_data['Hour'] <= 10)]\n",
    "\n",
    "\n",
    "# Handle duplicates by averaging the speeds for the same DateTime and ID\n",
    "filtered_data = filtered_data.groupby(['DateTime', 'ID']).agg({'H.Avg Speed': 'mean'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Set the maximum lag\n",
    "max_lag = 1  # This means we are considering 1 hour lag (4 * 15 minutes)\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag):\n",
    "    try:\n",
    "        dataframe = dataframe.dropna(subset=[column1, column2])\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[max_lag][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[max_lag][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Loop over the entire dataset for analysis\n",
    "outlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "inlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "\n",
    "# Color assignment based on sorted node IDs\n",
    "sorted_nodes = sorted(filtered_data['ID'].unique())\n",
    "color_palette = plt.cm.get_cmap('tab20', len(sorted_nodes))\n",
    "node_colors = {node: color_palette(i) for i, node in enumerate(sorted_nodes)}\n",
    "\n",
    "# Function to plot total strengths and save the plot\n",
    "def plot_total_strengths(total_strength, node_colors, title, ylabel, filename, dpi=300):\n",
    "    sorted_nodes = sorted(total_strength.items(), key=lambda x: x[1], reverse=True)\n",
    "    node_ids = [node[0] for node in sorted_nodes]\n",
    "    node_strengths = [node[1] for node in sorted_nodes]\n",
    "\n",
    "    # Ensure all node IDs have names\n",
    "    node_names = []\n",
    "    for node in node_ids:\n",
    "        name_row = merged_data[merged_data['ID'] == node]\n",
    "        if not name_row.empty:\n",
    "            node_names.append(name_row['Name'].iloc[0])\n",
    "        else:\n",
    "            node_names.append(f'Node {node}')\n",
    "\n",
    "    # Plot total strengths\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(np.arange(len(node_names)), node_strengths, color=[node_colors[node] for node in node_ids])\n",
    "\n",
    "    # Add hatch to \"IN\" nodes\n",
    "    for bar, node in zip(bars, node_ids):\n",
    "        direction = merged_data[merged_data['ID'] == node]['Direction'].iloc[0]\n",
    "        if direction == 'IN':\n",
    "            bar.set_hatch('*')\n",
    "\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.xticks(np.arange(len(node_names)), node_names, rotation=90, ha='right', fontsize=4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create legend based on IDDI part of node names\n",
    "    legend_labels = {}\n",
    "    for node_id, color in node_colors.items():\n",
    "        node_name = merged_data[merged_data['ID'] == node_id]['Name'].iloc[0] if not merged_data[merged_data['ID'] == node_id].empty else f'Node {node_id}'\n",
    "        if isinstance(node_name, str):\n",
    "            iddi_part = node_name.split(',')[0]  # Extract IDDI part of the name\n",
    "        else:\n",
    "            iddi_part = f'Node {node_id}'\n",
    "        if iddi_part not in legend_labels:\n",
    "            legend_labels[iddi_part] = color\n",
    "\n",
    "    plt.legend(handles=[Line2D([0, 1], [0, 0], color=color, linewidth=3) for color in legend_labels.values()],\n",
    "               labels=list(legend_labels.keys()), loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Save plot with specified DPI\n",
    "    plt.savefig(filename, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Pivot the filtered data\n",
    "pivot_data = filtered_data.pivot(index='DateTime', columns='ID', values='H.Avg Speed').fillna(0)\n",
    "\n",
    "links = {}\n",
    "outlink_strength = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "inlink_strength = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "\n",
    "# Analysis for each station pair\n",
    "for station1 in pivot_data.columns:\n",
    "    for station2 in pivot_data.columns:\n",
    "        if station1 != station2:\n",
    "            f_statistic_out = granger_test(pivot_data, station1, station2, max_lag)\n",
    "            if f_statistic_out > 0:\n",
    "                links[(station1, station2)] = f_statistic_out\n",
    "                outlink_strength[station1] += f_statistic_out\n",
    "                inlink_strength[station2] += f_statistic_out\n",
    "\n",
    "# Update total strengths\n",
    "for station in filtered_data['ID'].unique():\n",
    "    outlink_strength_total[station] += outlink_strength[station]\n",
    "    inlink_strength_total[station] += inlink_strength[station]\n",
    "\n",
    "# Merge COOR data with data to get coordinates\n",
    "merged_data = pd.merge(filtered_data, coor_data, left_on='ID', right_on='IDDI', how='left')\n",
    "\n",
    "# Plot total outlink strengths for the entire dataset and save the plot\n",
    "outlink_filename = 'plots/5.total_outlink_strength_MontoFri.jpg'\n",
    "plot_total_strengths(outlink_strength_total, node_colors, 'Total Outlink Strength per Node', 'Total Outlink Strength', outlink_filename)\n",
    "\n",
    "# Plot total inlink strengths for the entire dataset and save the plot\n",
    "inlink_filename = 'plots/5.total_inlink_strength_MontoFri.jpg'\n",
    "plot_total_strengths(inlink_strength_total, node_colors, 'Total Inlink Strength per Node', 'Total Inlink Strength', inlink_filename)\n",
    "\n",
    "# Print total strengths\n",
    "print(\"Total Outlink Strength:\", outlink_strength_total)\n",
    "print(\"Total Inlink Strength:\", inlink_strength_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d321f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adiancence matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import os\n",
    "\n",
    "# Ensure plots directory exists\n",
    "plots_dir = 'plots'\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('0.SpeedCT_15m.csv')\n",
    "\n",
    "# Load COOR data\n",
    "coor_data = pd.read_csv('CoorTraffic.csv')\n",
    "\n",
    "# Extract the start time from the 'Time' range\n",
    "data['StartTime'] = data['Time'].str.split('-').str[0]\n",
    "\n",
    "# Combine 'Date' and 'StartTime' to create 'DateTime'\n",
    "data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['StartTime'], format='%d.%m.%y %H:%M')\n",
    "\n",
    "# Drop the original 'Date' and 'Time' columns\n",
    "data.drop(columns=['Date', 'Time', 'StartTime'], inplace=True)\n",
    "\n",
    "# Set the maximum lag\n",
    "max_lag = 1  # This means we are considering 1 hour lag (4 * 15 minutes)\n",
    "\n",
    "# Filter for Tuesdays, Wednesdays, and Thursdays\n",
    "data['DayOfWeek'] = data['DateTime'].dt.dayofweek\n",
    "filtered_data = data[data['DayOfWeek'].isin([5])]  # 0=Monday, 1=Tuesday, 2=Wednesday, 3=Thursday, 4=Friday\n",
    "\n",
    "# Handle duplicates by averaging the speeds for the same DateTime and ID\n",
    "filtered_data = filtered_data.groupby(['DateTime', 'ID']).agg({'H.Avg Speed': 'mean'}).reset_index()\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag):\n",
    "    try:\n",
    "        dataframe = dataframe.dropna(subset=[column1, column2])\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[max_lag][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[max_lag][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Create node positions dictionary for plotting using the full coor_data\n",
    "node_positions = {row['IDDI']: (row['EST'], row['NORD']) for _, row in coor_data.iterrows()}\n",
    "\n",
    "# Initialize total strengths\n",
    "outlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "inlink_strength_total = {station: 0 for station in filtered_data['ID'].unique()}\n",
    "\n",
    "# Pivot the filtered data\n",
    "pivot_data = filtered_data.pivot(index='DateTime', columns='ID', values='H.Avg Speed').fillna(0)\n",
    "\n",
    "# Create an adjacency matrix DataFrame\n",
    "adj_matrix = pd.DataFrame(0, index=pivot_data.columns, columns=pivot_data.columns)\n",
    "\n",
    "# Analysis for each station pair\n",
    "for station1 in pivot_data.columns:\n",
    "    for station2 in pivot_data.columns:\n",
    "        if station1 != station2:\n",
    "            f_statistic_out = granger_test(pivot_data, station1, station2, max_lag)\n",
    "            adj_matrix.loc[station1, station2] = f_statistic_out\n",
    "\n",
    "# Save the adjacency matrix to a CSV file\n",
    "adj_matrix_csv_path = f'{plots_dir}/adjacency_matrix_Sat.csv'\n",
    "adj_matrix.to_csv(adj_matrix_csv_path)\n",
    "print(f\"Saved adjacency matrix to {adj_matrix_csv_path}\")\n",
    "\n",
    "# Print total strengths\n",
    "print(\"Total Outlink Strength:\", outlink_strength_total)\n",
    "print(\"Total Inlink Strength:\", inlink_strength_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
